<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-66428408-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-66428408-2');
</script>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>Capturing the motion of every joint: 3D Human Pose and Shape Estimation with independent tokens</title>
        <meta property="og:title" content="INT_HMR_Model" />
        <meta property="og:image" content="https://shunsukesaito.github.io/PIFu/resources/images/teaser.png" />
        <meta property="og:url" content="https://youtu.be/S1FpjwKqtPs" />
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">Capturing the Motion of Every Joint: 3D Human Pose and Shape Estimation with Independent Tokens</span>
    
    </center>

    <br><br>
      <table align=center width=800px>
       <tr>
         <td align=center width=100px>
         <center>
         <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=z5O3DLcAAAAJ&hl=zh-CN">Sen Yang<sup>1</sup></a></span>
         </center>
         </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="">Wen Heng<sup>2</sup></a></span>
        </center>
        </td>

        <td align=center width=100px>
          <center>
          <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=ZyzfB9sAAAAJ&hl=zh-CN&authuser=1">Gang Liu<sup>2</sup></a></span>
          </center>
        </td>
      <tr>
      </tr>
        <td align=center width=100px>
          <center>
          <span style="font-size:20px"><a href="https://github.com/guozhongluo">Guozhong Luo<sup>2</sup></a></span>
          </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://dblp.org/pid/99/3602.html">Wankou Yang<sup>1</sup></a></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://www.skicyyu.org/">Gang Yu<sup>2</sup></a></span>
        </center>
        </td>
     </tr>
    </table>

    <table align=center width=800px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:16px"><br/>Southeast University<sup>1</sup><br/>Tencent PCG<sup>2</sup><br/> </span>
        </center>
        </td>
     </tr>
    </table>

    <br>
    <table align=center width=400px>
     <tr>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px"><a href="https://openreview.net/forum?id=0Vv4H4Ch0la">[Paper]</a></span>
       </center>
       </td>

      <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="">[Video]</a></span>
      </center>
      </td>
       
      <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://github.com/yangsenius/INT_HMR_Model">[Code]</a></span>
      </center>
      </td>
      
   </tr>
  </table>

            <br>
            <br>

            <br>
              <div style="max-width:90%; margin:0 auto">
              In this paper we present a novel method to estimate 3D human pose and shape from monocular videos. This task requires directly recovering pixel-alignment 3D human pose and body shape from monocular images or videos, which is challenging due to its inherent ambiguity. 
To improve precision, existing methods highly rely on the initialized mean pose and shape as prior estimates and parameter regression with an iterative error feedback manner. In addition, video-based approaches model the overall change over the image-level features to temporally enhance the single-frame feature, but fail to capture the rotational motion at the joint level, and cannot guarantee local temporal consistency. 
To address these issues, we propose a novel Transformer-based model with a design of independent tokens. First, we introduce three types of tokens independent of the image feature: joint rotation tokens, shape token, and camera token. 
By progressively interacting with image features through Transformer layers, these tokens learn to encode the prior knowledge of human 3D joint rotations, body shape, and position information from large-scale data, and are updated to estimate SMPL parameters conditioned on a given image.
Second, benefiting from the proposed token-based representation, we further use a temporal model to focus on capturing the rotational temporal information of each joint, which is empirically conducive to preventing large jitters in local parts.
Despite being conceptually simple, the proposed method attains superior performances on the 3DPW and Human3.6M datasets. Using ResNet-50 and Transformer architectures, it obtains 42.0 mm error on the PA-MPJPE metric of the challenging 3DPW, outperforming state-of-the-art counterparts by a large margin.  
              </div>
          
         <!-- <table align=center width=550px> -->
            <!-- <table align=center width=800>
             <center><h1>Paper</h1></center>
                <tr>
                  <td><a href="https://arxiv.org/pdf/1905.05172.pdf"><img style="height:200px" src="./resources/images/paper.png"/></a></td>
                  <td><span style="font-size:14pt">Saito*, Huang*, Natsume*, Morishima, Kanazawa, Li.<br><br>
                    PIFu: Pixel-Aligned Implicit Function for </br>High-Resolution Clothed Human Digitization.<br><br>
                    ICCV 2019.<br><br>
                      <a href="https://arxiv.org/pdf/1905.05172.pdf">[pdf]</a> &nbsp; &nbsp;
                    <a href="https://shunsukesaito.github.io/PIFu/resources/bibtex.txt">[Bibtex]</a>
                    </td>
              </tr>
            </table> -->
          <br>

                <hr>
                <!-- <center><h1>Paper Video</h1></center>
                <table align=center width=900px>
                    <tr>
                        <td width=600px>
                          <center>
                            <div class = "video">
                              <iframe width="720" height="405" src="https://youtube.com/embed/S1FpjwKqtPs" frameborder="0" allowfullscreen></iframe>
                           </div>
                        </center>
                        </td>
                    </tr>
                </table> -->
                <br>
               
              <center>
                  <div class = "video">
                  
                    <video src="supplementary_videos/micheal5_contrast_s.mp4" style="max-width:70%;" playsinline autoplay loop preload muted></video>
                  </div>
                  <br>
                  *Based on the same detection and tracking framework provided by VIBE demo
                
                </center>
                <br>
                <center>
                  <div class = "video">
                    <video src="supplementary_videos/dance5_contrast_s.mp4" style="max-width:70%;" playsinline autoplay loop preload muted></video>
                   
                  </div> <br>
                  *Based on the same detection and tracking framework provided by VIBE demo
              </center>
  
             <center>
                  <div class = "video">
                    <video src="supplementary_videos/cxk_int_result.mp4" style="max-width:70%;" playsinline autoplay loop preload muted></video>

                  </div> <br>

              </center>

                <center><h1>Single-person Human Mesh Reconstruction</h1></center>
                <table align=center width=1000px>
                    <tr>
                        <td width=1000px>
                          <center>
                            <div class = "video">
                              <video src="supplementary_videos/contrast_1.mov" style="max-width:80%;" playsinline autoplay loop preload muted></video>
                           </div>

                            <div class = "video">
                            <video src="supplementary_videos/contrast_out_slalom.mov" style="max-width:40%;" playsinline autoplay loop preload muted></video>
      
                            <video src="supplementary_videos/contrast_outdoors_climbing.mov" style="max-width:40%;" playsinline autoplay loop preload muted></video>
                          </div>
                          </center>
                        </td>
                    </tr>
                </table>
                <br>
                <hr>
   
                <center><h1>Multi-person Human Mesh Reconstruction</h1></center>
                <table align=center width=1000px>
                    <tr>
                        <td width=1000px>
                          <center>
                            <div class = "video">
                          
                            
                              <video src="supplementary_videos/contrast_downtown_runForbus2.mov" style="max-width:80%;" playsinline autoplay loop preload muted></video>
                              <!-- <video src="supplementary_videos/contrast_posetrack.mp4" style="max-width:40%;" playsinline autoplay loop preload muted></video> -->

                            </div>
                        </center>
                        </td>
                    </tr>
                </table>
                <br>
                <hr>

                
                <br>
              

                
                <br>
               
<!-- 
         <center><h1>Code</h1></center>
            <table align=center width=1000px>
                <tr>
                        <center>
                          <a href='https://github.com/shunsukesaito/PIFu'><img class="round" style="height:250" src="./resources/images/overview.png"/></a>
                        </center>
              </tr>
          </table>

            <table align=center width=800px>
              <tr><center> <br>
                <span style="font-size:28px">&nbsp;<a href='https://github.com/shunsukesaito/PIFu'>[GitHub]</a>
                  
                <span style="font-size:28px"></a></span>
              <br>
              </center></tr>
          </table>
            <br>
          <hr> -->


            <table align=center width=1000px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                    <div style="max-width: 90%; margin:0 auto">
                      This work was supported by the National Natural Science Foundation of China under Nos. 62276061, 61773117 and 62006041. This webpage template was borrowed from <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
                    </div>
                </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
